# Datasets For Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations

The datasets associated with this paper are organized by type (where they are created and how they are used).

- **Data Inputs** are datasets which must be downloaded from an external source, or they may be included in this repository. They are ingested by the codebase.
- **Data Artifacts** are temporary datasets which can be re-generated from the codebase, and may or may not be available for download and may or may not be documented.
- **Data Outputs** are the datasets generated by this codebase which we will document and make available for download.

Each documented dataset is listed below, organized by type. Following the listing are schemas for each documented dataset.

## Datasets

Those marked "Input" must be placed in the `/data_inputs` directory and unzipped in order to run the code.

| type | name | descrption | size (compressed) | format | License | DOI / Ref / Attribution | Direct Download Link |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Input | [butter_e_energy.zip](#butter_e_energyzip) | 1-minute raw time series power data | 20.9 MB | CSV | [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) | [OEDI Butter-E 5991](https://data.openei.org/submissions/5991)  | [Download](https://data.openei.org/files/5991/butter_e_energy.zip) |
| Input | [butter_e_metadata.csv.zip](#butter_e_metadatacsvzip) | Metadata concerning each training run | 6.2 MB | CSV | [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) |  [OEDI Butter-E 5991](https://data.openei.org/submissions/5991) |   [Download](https://data.openei.org/files/5991/butter_e_metadata.csv.zip) |
| Input | [summary_by_epoch.zip](#summary_by_epochtar) | Training losses re-summarized from the BUTTER dataset | 1.27 GB | Parquet | [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) | [OEDI Butter-E 5991](https://data.openei.org/submissions/5991) |  [Download](https://data.openei.org/files/5991/summary_by_epoch.tar) |
| Input | [node_power_dist.csv](#butter_e_node_power_distcsv) | power consumption quantiles for each node | 1.8 MB | CSV | [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) | [OEDI Butter-E 5991](https://data.openei.org/submissions/5991) | [Download](https://data.openei.org/files/5991/node_power_dist.csv) |
| Input | [node_sinfo.csv](#slurm_node_sinfocsv) | Characteristics of each compute node | 97 KB | CSV | [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) | [OEDI Butter-E 5991](https://data.openei.org/submissions/5991) | [Download](https://data.openei.org/files/5991/node_sinfo.csv) |
| Input | [NVIDIA_GPU_Processors_curated.csv](#nvidia_gpu_processors_curatedcsv) | Metadata concerning historical NVIDIA GPUs | 24 KB | CSV | [CC-BY-SA](https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License) | Scraped from [Wikipedia](https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units). Included in this repo. | [Download](data_inputs/NVIDIA_GPU_Processors_curated.csv) |
| Input | [amd_ryzen_processors.csv](#amd_ryzen_processorscsv) | Metadata concerning historical AMD Processors | 9KB | CSV | [CC-BY-SA](https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License) | Scraped from [Wikipedia](https://en.wikipedia.org/wiki/List_of_AMD_Ryzen_processors) Included in this repo. | [Download](data_inputs/amd_ryzen_processors.csv) |
| Output | [runs_with_standardized_energy.csv.zip](#runs_with_standardized_energycsvzip) | Power data joined to run data | 5.8 MB | CSV | [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) | [OEDI Butter-E 5991](https://data.openei.org/submissions/5991) | [Downoad](https://data.openei.org/files/5991/runs_with_standardized_energy.csv.zip)  |

### External Datasets

| type | name | descrption | size (compressed) | format | DOI / Ref | License | Link |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Input | cpudb.zip | Historical CPU Metadata | 745 KB | CSV | 10.1145/2133806.2133822 |  Unknown | [External Link](http://cpudb.stanford.edu) |
| Input | pmlb.csv | Metadata about each training dataset | 19 KB | CSV | 10.1186/s13040-017-0154-4 | MIT License | [External Link](https://github.com/EpistasisLab/pmlb/blob/master/pmlb/all_summary_stats.tsv) |
| Input | intel-processors.zip | Historical performance data about intel processors | 197 KB | CSV | Unknown |  GPL-3 | [External Link](https://github.com/toUpperCase78/intel-processors) | 
| Input | ML_model_details.xlsx | Spreadsheet containing details of historical machine learning models. | 1MB | Excel | Epoch, Parameter, Compute and Data Trends in Machine Learning. 2022. | CC-BY 4.0 | [External Link](https://epochai.org/data/pcd) |


# Schemas for Each Dataset

## butter_e_energy.zip
1-minute raw time series power data from the NREL hpcmon api partitioned by SLURM Job IDs for each run in the Butter-E dataset. Zipped folder contains ~3,200 CSV files of spanning ~200MB (uncompressed).
Code to re-generate these CSVs is in 1_download.ipynb

### Columns
- NAME OF CSV FILE: {slurm_job_id}.csv 
    - The CSV file name is the Slurm Job ID.
- timestamp: [str], Time at which the measurement was taken. *Note, this timestamp is recorded in local, system time, which was America/Denver. The time zone attached to this timestamp is incorrect and related to a bug in an internal API used to generate the data.*
- node: [str], hostname of the node for which the measurement was taken.
- watts: [int], Measured power, in watts, being consumed by the node.


## butter_e_metadata.csv.zip

Result of a query run on yuma.hpc.nrel.gov on 11/17/2023 downloading metadata for all runs in the Butter-E dataset.
The query is stored in butter_e_metadata.sql and also described in download.ipynb.

Columns ending with an asterisk (\*) denote columns which are identical in definition and values to those in the BUTTER dataset.

### Columns
- run_id: [str], uuid of run
- experiment_id: [str], uuid of parent experiment which generated this run
- old_experiment_id: [int], legacy experiment id, which was an integer primary key
- slurm_job_id: [int], id of the slurm job which executed the run
- start_time: [str], system time when the run began to execute
- update_time: [str], system time when the last epoch of the run was executed
- host_name: [str], node name that executed this run on the Eagle HPC system
- size: [int], approximate number of trainable parameters used\*
- depth: [int], number of layers\*
- shape: [str], network shape\*
- dataset: [str], name of the dataset used\*
- learning_rate: [str], the learning rate used\*
- batch_size: [str], the batch size used\*
- optimizer: [str], name of the opimizer used\*
- is_gpu: [bool], Run was executed on a node with GPUs (and it used the GPUs).
- run_data: [str], JSON object containing more information about this run
- batch: [str], name of the batch of experiments to which this run belongs

## summary_by_epoch.tar
Re-summarization of data from the BUTTER dataset, but using statistics produced from the best epoch *so far* at that epoch.

### File Heirarchy
The dataset is partitioned using the HIVE partitioning scheme, with nested folders named "{column}={value}". These partition keys are utilized by libraries that support parquet datasets, such as pyarrow.

For some columns, multiple columns are condensend into one entry using curly brackets to denote a variable in the column name. For example, the entry for column ``test_{metric}_quantile_{q}`` denotes a family of columns matching this pattern, for example: ``test_loss_quantile_50``.

Columns ending with an asterisk (\*) denote columns which are identical in definition and values to those in the BUTTER dataset.

### Columns

- dataset: [str] **partition key**, name of the dataset used\*
- shape: [str] **partition key**, network shape\*
- optimizer: [str] **partition key**, name of the opimizer used\*
- learning_rate: [float] **partition key**, learning rate used\*
- batch_size: [int] **partition key**, size of minibatches used\*
- regularizer: [str] **partition key**, name of regularizer used\*
- has_label_noise: [bool] **partition key**, if the run has any added label noise\*
- depth: [int], number of layers in network\*
- epochs: [int], *Always 0. This column was accidentally included in the dataset and may be ignored. Use the column 'epoch' instead.*
- experiment_id [int], legacy experiment id
- num_free_parameters [int], actual number of free parameters used in model\*
- widths [array[int]], list of integers corresponding to the number of neurons (also known as units) in each layer.
- num_runs [int], count of the number of matching runs for the given hyperparameters.
- label_noise [float], proportion of the number of labels which were augmented\*
- momentum [float], momentum parameter passed to optimizer\*
- size [int], approximate number of free parameters in model\*
- task [str], classification or regression, depending on the dataset\*
- l1 [float], parameter of the l1 regularizer
- l2 [float], parameter of the l2 regularizer
- epoch: [array[int]], epoch number corresponding to the metric values at the same position in the following metric array columns in this table. 
- test_{metric}\_quantile\_{q}: [float], The q^th percentile value of the metric, for the test set, at this epoch, over all repetitions.
    - metric: loss, accuracy, mean_squared_error, binary_crossentropy, categorical_crossentropy
    - q: 0,25,50,75,100
- test_{metric}\_best\_quantile\_{q}: [float], The q^th percentile value of the metric, for the test set, at the best epoch *so far*, over all repetitions. See metric and quantile values from prevoious entry in list.
- train_{metric}\_quantile\_50: [float], The median value of the metric, for the training set, at this epoch, over all repetitions. See metric values from prevoious entry in list.
- train_{metric}\_best\_epoch\_quantile\_50: [float], The median value of the metric, for the traning set, at the best epoch *so far*, over all repetitions. See metric values from prevoious entry in list.


## butter_e_node_power_dist.csv
This CSV file contains the power consumption quantiles for each node of the Eagle HPC system used to create the BUTTER-E dataset during a two month time period around the time of the BUTTER-E dataset creation. The purpose of this dataset is to estimate the power consumption of a given node at idle, to help standardize the power readings across nodes.

### Columns:

- node: [str], hostname of the Eagle node
- count: [int], number of power readings included in the summary for this node
- p{q}: [int], approximate q^th percentile of power consumption in watts.
    - q should be interpreted as a percentile. Leading zeros represent a decimal place. Trailing 9s represent 9s after the decimal place. For example:
        - p0001 is the 0.0001 (*100) = 0.01 percentile.
        - p001 is the 0.001 (*100) = 0.1 percentile.
        - p1 is the 0.01 (*100) = 1 percentile.
        - p99 is the 0.99 (*100) = 99 percentile.
        - p999 is the 0.999 (*100) = 99.9 percentile

## slurm_node_sinfo.csv

This file contains information about each node in NREL's Eagle HPC System used to generate the dataset.

### Columns:

- HOSTNAMES: [str], node name on the Eagle HPC system
- CPUS: [int], number of CPU cores on this node
- MEMORY: [int], total system memory on this node (MB)
- TMP_DISK: [int], total scratch-disk space on this node (MB)
- GRES: [str], type of GPU co-processor on this node, if any.


## pmlb.csv

This file contains metadata about each training dataset (and some more) used to generate the BUTTER-E dataset. These data are reproduced from the PMLB benchmark dataset.

### Columns:

- Dataset: [str], name of dataset matching that in PMLB.
- n_observations: [str], number of observations in this dataset.
- n_features: [str], number of features per observation.
- n_classes: [str], number of classes if task is classification.
- Endpoint: [str], type of output (categorical vs continuous).
- Imbalance: [str], class imbalance.
- Task: [str], machine learning task (classification vs regression).
- Metadata: This column is empty. ???

## runs_with_standardized_energy.csv.zip

This file is the output of notebook 4_standardize.ipynb. It contains the run metadata from butter_e_metadata.csv joined with the power consumption data from butter_e_energy.csv, integrated over the time period of each run. This file also contains several filter columns added by 3_qc.ipynb, and a standardized energy columns added by 4_standardize.ipynb which corrects for varying power consumption of nodes at idle.

### Columns:
- run_id: [str], uuid of run
- experiment_id: [str], uuid of parent experiment which generated this run
- slurm_job_id: [int], id of the slurm job which executed the run
- start_time: [str], system time when the run began to execute
- update_time: [str], system time when the last epoch of the run was executed
- node: [str], node name that executed this run on the Eagle HPC system
- size: [int], approximate number of trainable parameters used\*
- depth: [int], number of layers\*
- shape: [str], network shape\*
- dataset: [str], name of the dataset used\*
- learning_rate: [str], the learning rate used\*
- batch_size: [str], the batch size used\*
- optimizer: [str], name of the opimizer used\*
- is_gpu: [bool], Run was executed on a node with GPUs (and it used the GPUs).
- batch: [str], name of the batch of experiments to which this run belongs
- energy: [float], The energy consumption of the run as computed by integration over the 1-minute power data. Linear interpolation is used at the endpoints, as they do not fall precisely on the 1-minute boundary in general.
- run_time: [float], difference between start time and update time in seconds. 
- filter_1: [bool], Is the computed runtime greater than 75,000 seconds.
- filter_2: [bool], Is the computed energy zero? 
- filter_3: [bool], Is the standard deviation of the energy of this run greater than 4 from the mean of similar runs?
- filter_3_stdevs: [float], The standard deviation values used to compute Filter 3.
- filter: [bool], The logical disunction (or) of all filters.
- start_time_int: **TBD**
- start_offset: **TBD**
- power: **TBD**
- num_reps: **TBD**
- std_power: **TBD**
- std_energy: **TBD**
- energy_overhead: **TBD**
- non_overhead_energy: **TBD**
- runtime_overhead: **TBD**
- non_overhead_runtime: **TBD** 

## NVIDIA_GPU_Processors_curated.csv

Metadata regarding historical NVIDIA GPU Processors. This data was scraped from Wikipedia. The original URL used to scrape the data was:
https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units. At the time of access, the page text is licensed under [Creative Commons Attribution-ShareAlike License 4.0.](https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License)

### Columns:
- model: [str], Name of product or model name.
- launch: [str], Date of launch for this product.
- Fillrate Pixel (GP/s): [float], Pixel fillrate; in computer graphics, a video card's pxel fillrate refers to the number of pixels that can be rendered on the screen and written to video memory in one second (provided here in units of gigapixels per second).
- Fillrate Texture (GT/s): [float], Texture fillrate; it refers to the number of texture map elements (texels)the GPU can map to pixels in one second (provided here in units of gigatexels per second).
- Processing power (GFLOPS) Single precision: [float],  Single-precision processing power measured in giga floating point operations per second
- TDP (Watts): [int], Thermal Design Power in Watts

## amd_ryzen_processors.csv

Metadata regarding historical AMD Processors. This data was scraped from Wikipedia. The original URL used to scrape the data was:
https://en.wikipedia.org/wiki/List_of_AMD_Ryzen_processors. At the time of access, the page text is licensed under [Creative Commons Attribution-ShareAlike License 4.0.](https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License)


### Columns:
- Branding and Model: [str], Name of product or model name.
- Releasedate: [str], Date of launch for this product.
- Clock (GHz) Base: [float], Clock rate or clock speed in GHz
- Cores (Threads): [str], Number of cores and corresponding threads
- Cores: [int], Number of threads
- Threads: [int], Number of cores
- TDP: [float],  Thermal Design Power in Watts
